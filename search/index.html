<!DOCTYPE html>
<html lang="en">
<head>
<title>Search :: RoundofThree</title>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="" name="description"/>
<meta content="[blog pwn ctf morello cheri memory-safety]" name="keywords"/>
<meta content="noodp" name="robots"/>
<link href="/search/" rel="canonical"/>
<link href="/assets/style.css" rel="stylesheet"/>
<link href="/assets/red.css" rel="stylesheet"/>
<link href="/img/apple-touch-icon-192x192.png" rel="apple-touch-icon"/>
<link href="/favicon.png" rel="shortcut icon"/>
<meta content="summary" name="twitter:card"/>
<meta content="" name="twitter:site"/>
<meta content="" name="twitter:creator"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="Search" property="og:title"/>
<meta content="" property="og:description"/>
<meta content="/search/" property="og:url"/>
<meta content="RoundofThree" property="og:site_name"/>
<meta content="/favicon.png" property="og:image"/>
<meta content="2048" property="og:image:width"/>
<meta content="1024" property="og:image:height"/>
<link href="/search/index.xml" rel="alternate" title="RoundofThree" type="application/rss+xml"/>
</head>
<body class="red">
<div class="container center headings--one-size">
<header class="header">
<div class="header__inner">
<div class="header__logo">
<a href="/">
<div class="logo">
    RoundofThree
  </div>
</a>
</div>
<div class="menu-trigger">menu</div>
</div>
<nav class="menu">
<ul class="menu__inner menu__inner--desktop">
<li><a href="/general">General</a></li>
<li><a href="/labs">Labs</a></li>
<li><a href="/research">Research</a></li>
<li><a href="/">Whoami</a></li>
</ul>
<ul class="menu__inner menu__inner--mobile">
<li><a href="/general">General</a></li>
<li><a href="/labs">Labs</a></li>
<li><a href="/research">Research</a></li>
<li><a href="/">Whoami</a></li>
</ul>
</nav>
</header>
<form action="/search/" id="search" method="get">
<label for="search-input" hidden="">Search site</label>
<input id="search-input" name="query" placeholder="Type here to search" type="text"/>
<input type="submit" value="search"/>
</form>
<div class="content">
<ul id="results">
<li>
        Enter a keyword above to search this site.
    </li>
</ul>
<script>
    window.store = {
        
        
        
        
        "\/tags\/cheri\/": {
            
            "title": "cheri",
            "tags": [],
            "content": "", 
            "url": "\/tags\/cheri\/"
        },
        
        
        
        "\/labs\/gef-cheri\/": {
            
            "title": "GEF for CheriBSD Morello",
            "tags": ["tooling","cheri",],
            "content": "Why the need for this? Because having some handy commands to immediately generate a CLI visualization is very helpful when debugging a binary for exploit developers (or software development). For example, we can quickly get an idea of the state of the heap, which speeds up the process of debugging heap-based exploits. gef-cheri enables this for the CheriBSD platform in the Morello architecture (CHERI-enabled ARM64). You can still apply the same gef-cheri script to analyse non-CHERI binaries, in which case the behavior should be the same as the original gef.\nSetup To load the gef-cheri python script when you start a debugging session, add this line to ~/.gdbinit:\nsource /path/to/gef-cheri/gef.py Some commands Here are some commands that I often use.\nvmmap Getting process memory mappings information is different in FreeBSD (and CheriBSD) and in Linux. gef-cheri is taught to identify a platform other than Linux. In addition, the original gef identifies the heap memory mappings by assuming that heap memory is managed by the glibc memory allocator. This is not true in FreeBSD (and CheriBSD), so we have to implement a heap memory blocks identification logic for each heap manager other than glibc malloc. Below is the output of the vmmap command against a binary in CheriBSD running the jemalloc memory allocator.\ngef\u0026gt; vm [ Legend: Code | Heap | Stack ] Start End Offset Perm Path 0x0000000000100000 0x0000000000101000 0x0000000000000000 r-- /home/roundofthree/heap_game 0x0000000000101000 0x0000000000110000 0x0000000000001000 --- 0x0000000000110000 0x0000000000111000 0x0000000000000000 r-x /home/roundofthree/heap_game 0x0000000000111000 0x0000000000120000 0x0000000000011000 --- 0x0000000000120000 0x0000000000122000 0x0000000000000000 r-- /home/roundofthree/heap_game 0x0000000000122000 0x0000000000131000 0x0000000000022000 --- 0x0000000000131000 0x0000000000132000 0x0000000000000000 rw- 0x0000000040131000 0x0000000040139000 0x0000000000000000 r-- /libexec/ld-elf.so.1 0x0000000040139000 0x0000000040148000 0x0000000000008000 --- 0x0000000040148000 0x0000000040163000 0x0000000000007000 r-x /libexec/ld-elf.so.1 0x0000000040163000 0x0000000040172000 0x0000000000032000 --- 0x0000000040172000 0x0000000040175000 0x0000000000021000 rw- /libexec/ld-elf.so.1 0x0000000040175000 0x0000000040184000 0x0000000000044000 --- 0x0000000040184000 0x0000000040185000 0x0000000000023000 rw- /libexec/ld-elf.so.1 0x0000000040185000 0x0000000040187000 0x0000000000000000 rw- 0x0000000040187000 0x0000000040190000 0x0000000000002000 rw- 0x0000000040191000 0x0000000040223000 0x0000000000000000 r-- /lib/libc.so.7 0x0000000040223000 0x0000000040232000 0x0000000000092000 --- 0x0000000040232000 0x0000000040370000 0x0000000000091000 r-x /lib/libc.so.7 0x0000000040370000 0x000000004037f000 0x00000000001df000 --- 0x000000004037f000 0x000000004039c000 0x00000000001ce000 r-- /lib/libc.so.7 0x000000004039c000 0x00000000403ab000 0x000000000020b000 --- 0x00000000403ab000 0x00000000403b6000 0x00000000001ea000 rw- /lib/libc.so.7 0x00000000403b6000 0x00000000407e7000 0x0000000000000000 rw- 0x00000000407e7000 0x00000000407e9000 0x0000000000000000 rw- 0x00000000407e9000 0x00000000407f0000 0x0000000000000000 --- 0x00000000407f0000 0x0000000040811000 0x0000000000000000 rw- 0x0000000040811000 0x0000000040818000 0x0000000000000000 --- 0x0000000040818000 0x0000000040828000 0x0000000000000000 rw- 0x0000000040828000 0x0000000040877000 0x0000000000010000 rw- 0x0000000040a00000 0x0000000040c00000 0x0000000000000000 rw- [heap block] 0x0000000040c00000 0x0000000040e00000 0x0000000000200000 rw- [heap] 0x0000000040e00000 0x0000000041400000 0x0000000000400000 rw- [heap block] 0x0000fbfdbffff000 0x0000fbfdc0000000 0x0000000000000000 rw- 0x0000fbfdc0000000 0x0000fe0000000000 0x0000000000000000 rw- 0x0000ffffbfeff000 0x0000ffffbff80000 0x0000000000000000 rw- 0x0000ffffbff80000 0x0000fffffff60000 0x00000000001b001b --- 0x0000fffffff60000 0x0000fffffff80000 0x0000000000000000 rw- [stack] 0x0000fffffffff000 0x0001000000000000 0x0000000000000000 r-x telescope One notable difference is that the stack alignment is 0x10 in CHERI-enabled platforms. By default, telescope without arguments will display a stack view starting from the address pointed by $csp ($rcsp if in Restricted mode, see CHERI ISAv9). We also can use the capability tag as a filter to indicate whether to dereference or not, it\u0026rsquo;s more accurate than guessing whether a value is a code or data pointer.\ngef\u0026gt; telescope 0x0000fffffff7f800│+0x0000: 0x0000fffffff7f820 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7f850 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7fd00 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7fe60 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7ff60 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7ff80 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7ffe0 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000]\t← $c29, $csp 0x0000fffffff7f810│+0x0010: 0x00000000402bc9d9 [rxRX,0x40191000-0x407e7000,len=0x656000] (sen) → \u0026lt;_sread+0020\u0026gt; cmp w0, #0x1 0x0000fffffff7f820│+0x0020: 0x0000fffffff7f850 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7fd00 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7fe60 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7ff60 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7ff80 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7ffe0 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000000000000000 0x0000fffffff7f830│+0x0030: 0x00000000402bbfa1 [rxRX,0x40191000-0x407e7000,len=0x656000] (sen) → \u0026lt;__srefill+0154\u0026gt; ldrh w8, [c19, #24] 0x0000fffffff7f840│+0x0040: 0x00000000403adfa0 [rwRWX,0x403adfa0-0x403ae510,len=0x570] → 0x0000000040c17000 [rwRW,0x40c17000-0x40c18000,len=0x1000] → 0x00000000000a0a31 (\u0026#34;1\\n\\n\u0026#34;?) 0x0000fffffff7f850│+0x0050: 0x0000fffffff7fd00 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7fe60 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7ff60 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7ff80 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7ffe0 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000000000000000 0x0000fffffff7f860│+0x0060: 0x00000000402c11d9 [rxRX,0x40191000-0x407e7000,len=0x656000] (sen) → \u0026lt;__svfscanf+0538\u0026gt; cbnz w0, 0x402c23ec \u0026lt;__svfscanf+5964\u0026gt; 0x0000fffffff7f870│+0x0070: 0x0000000000000000 0x0000fffffff7f880│+0x0080: 0x0000fffffff7f967 [rwRW,0xfffffff7f968-0xfffffff7fb69,len=0x201] → 0x5d40007c50fb2000 0x0000fffffff7f890│+0x0090: 0x0000fffffff7f9d0 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7fc10 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7fdc0 [rwRW,0xffffbff80000-0xfffffff80000,len=0x40000000] → 0x0000fffffff7fea0 [rwRW,0xfffffff7fea0-0xfffffff7feb0,len=0x10] → 0x0000fffffff7ff24 [rwRW,0xfffffff7ff24-0xfffffff7ff28,len=0x4] → 0x0000000100000001 gef\u0026gt; 0x0000fffffff7f8a0│+0x00a0: 0x0000fffffff7fc70 [rwRW,0xfffffff7fc70-0xfffffff7fcf0,len=0x80] → 0x0000000000001800 0x0000fffffff7f8b0│+0x00b0: 0x0000fffffff7fa00 [rwRW,0xfffffff7fa00-0xfffffff7fa80,len=0x80] → 0xfffffffffffffffffb5d25837d7ff700 0x0000fffffff7f8c0│+0x00c0: 0x0000000a0611487b 0x0000fffffff7f8d0│+0x00d0: 0x00000000403ae170 [rwRWX,0x403adfa0-0x403ae510,len=0x570] → 0x0000000040c16000 [rwRW,0x40c16000-0x40c17000,len=0x1000] → \u0026#34;Has chunk 2 been freed? (1 for Yes, 0 for No): \u0026#34; 0x0000fffffff7f8e0│+0x00e0: 0x0000000000000001 0x0000fffffff7f8f0│+0x00f0: 0x00000000403ad5a0 [rwRWX,0x403ad5a0-0x403ad700,len=0x160] → 0x0000000000000000 0x0000fffffff7f900│+0x0100: 0x000000004081a240 [rwRW,0x4081a240-0x4081a250,len=0x10] → 0x0000000000000000 0x0000fffffff7f910│+0x0110: 0x0000000000000000 0x0000fffffff7f920│+0x0120: 0x0000fffffff7fb6c [rwRW,0xfffffff7fb6c-0xfffffff7fc6c,len=0x100] → 0xfffffffffffffffffff7fb1fdc5d4000 0x0000fffffff7f930│+0x0130: 0x0000ffffffffffff [-,0xffffbff80000-0xfffffff80000,len=0x40000000] (bad) xinfo This is exactly the same command in the original gef script.\ngef\u0026gt; xinfo 0x40c28000 ──────────────────────────────────────────────────────────────────────────────────────────────── xinfo: 0x40c28000 ──────────────────────────────────────────────────────────────────────────────────────────────── Page: 0x0000000040c00000 → 0x0000000040e00000 (size=0x200000) Permissions: rw- Pathname: [heap] Offset (from page): 0x28000 Inode: 0 scancap gef-cheri implements the needle-in-haystack search with an additional filter of valid capabilities (valid tag). For example, the example below scans for valid capabilities in the stack memory region that points to the heap memory region.\ngef\u0026gt; scan stack heap [+] In \u0026#39;[stack]\u0026#39; (0xfffffff60000-0xfffffff80000), permission=rw- 0xfffffff7cb50 → 0x00000040c0e000 [rwRW,0x40c00000-0x40e00000,len=0x200000] 0xfffffff7df80 → 0x00000040c1f000 [rwRW,0x40c00000-0x40e00000,len=0x200000] 0xfffffff7e880 → 0x00000040c28000 [rwRW,0x40c00000-0x40e00000,len=0x200000] 0xfffffff7f6a0 → 0x00000040c16000 [rwRW,0x40c16000-0x40c17000,len=0x1000] 0xfffffff7fb50 → 0x000000001009b1 [rR,0x100993-0x1009b4,len=0x21] 0xfffffff7fdb0 → 0x00000000100945 [rR,0x100945-0x100949,len=0x4] 0xfffffff7fe70 → 0x00000000110d35 [rxRX,0x100000-0x1313c0,len=0x313c0] (sen) 0xfffffff7fe90 → 0x00000000110b8d [rxRX,0x100000-0x1313c0,len=0x313c0] (sen) 0xfffffff7feb0 → 0x00000040c28000 [rwRW,0x40c28000-0x40c28010,len=0x10] 0xfffffff7ff40 → 0x00000040c20000 [rwRW,0x40c20000-0x40c200a0,len=0xa0] 0xfffffff7ff90 → 0x00000000110b59 [rxRX,0x100000-0x1313c0,len=0x313c0] (sen) More heap managers To analyze heaps managed by allocators other than glibc allocator, I developed some plugins in gef-plugins repo. To load it at the start of a debugging session, add this line to ~/.gdbinit:\ngef config gef.extra_plugins_dir /path/to/gef-plugins jemalloc  jheap chunks: list in use heap allocations  gef\u0026gt; jheap chunks Chunk(addr=0x40c00000, size=0xe000 (Large), status=Used) [0x0000000040c00000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................] Chunk(addr=0x40c0e000, size=0x8 (Small), status=Used) [0x0000000040c0e000 e2 03 00 00 e8 03 00 00 00 00 00 00 00 00 00 00 ................] Chunk(addr=0x40c0f000, size=0xe0 (Small), status=Used) [0x0000000040c0f000 70 f0 c0 40 00 00 00 00 00 f0 d0 70 00 40 5d dc p..@.......p.@].] Chunk(addr=0x40c16000, size=0x1000 (Small), status=Used) [0x0000000040c16000 48 61 73 20 63 68 75 6e 6b 20 32 20 62 65 65 6e Has chunk 2 been] Chunk(addr=0x40c17000, size=0x1000 (Small), status=Used) [0x0000000040c17000 31 0a 0a 00 00 00 00 00 00 00 00 00 00 00 00 00 1...............] Chunk(addr=0x40c20000, size=0xa0 (Small), status=Used) [0x0000000040c20000 00 50 c2 40 00 00 00 00 07 50 07 30 00 40 5d dc .P.@.....P.0.@].] Chunk(addr=0x40c25000, size=0x6000 (Large), status=Used) [0x0000000040c25000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................] Chunk(addr=0x40c2b000, size=0x10000 (Large), status=Used) [0x0000000040c2b000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................]  jheap chunk \u0026lt;address\u0026gt;: inspect a heap allocation  gef\u0026gt; jheap chunk 0x40c40000 Chunk(addr=0x40c40000, size=0x4000 (Large), status=Free) In quarantine: Yes In tcache: No Extent base: 0x40c40000 Extent size: 0x4000  jheap uaf [noheap]: scan for freed heap allocations that are pointed by valid capabilities in memory. Optionally, exclude capabilities stored in the heap.  gef\u0026gt; jheap uaf noheap [+] In \u0026#39;[stack]\u0026#39; (0xfffffff60000-0xfffffff80000), permission=rw- 0xfffffff7df80 → 0x00000040c1f000 [rwRW,0x40c00000-0x40e00000,len=0x200000] snmalloc  snheap info: print pagemap address snheap localcache: list entries in local cache LocalCache (also called small fast free lists) snheap slabs: lists slabs in the core allocator (there can be multiple slabs per small size class, and large slabs) snheap remote: lists the remote deallocation queue of the current of given thread(s) snheap freelists: list entries in local cache LocalCache in the local allocator, the deallocation queue in remote allocators and active slab free lists in the core allocator  gef\u0026gt; snheap freelists [+] Guessed the LocalAlloc address to be 0x40a42070 ────────────────────────────────────────────────────────────────────────────── Thread 1: localcache for LocalAlloc(addr=0x40a42070) ────────────────────────────────────────────────────────────────────────────── small_fast_free_lists[idx=2, size=0x60, count=169] → Alloc(addr=0x42024060) → Alloc(addr=0x420240c0) → Alloc(addr=0x42024120) → Alloc(addr=0x42024180) → ... → Alloc(addr=0x42027f60) small_fast_free_lists[idx=3, size=0x80, count=127] → Alloc(addr=0x42008080) → Alloc(addr=0x42008100) → Alloc(addr=0x42008180) → Alloc(addr=0x42008200) → ... → Alloc(addr=0x4200bf80) small_fast_free_lists[idx=4, size=0xa0, count=101] → Alloc(addr=0x420200a0) → Alloc(addr=0x42020140) → Alloc(addr=0x420201e0) → Alloc(addr=0x42020280) → ... → Alloc(addr=0x42023f20) small_fast_free_lists[idx=6, size=0xe0, count=72] → Alloc(addr=0x420100e0) → Alloc(addr=0x420101c0) → Alloc(addr=0x420102a0) → Alloc(addr=0x42010380) → ... → Alloc(addr=0x42013f00) small_fast_free_lists[idx=20, size=0xa00, count=5] → Alloc(addr=0x42004a00) → Alloc(addr=0x42005400) → Alloc(addr=0x42005e00) → Alloc(addr=0x42006800) → ... → Alloc(addr=0x42007200) small_fast_free_lists[idx=23, size=0x1000, count=3] → Alloc(addr=0x42015000) → Alloc(addr=0x42016000) → Alloc(addr=0x42017000) small_fast_free_lists[idx=30, size=0x3800, count=3] → Alloc(addr=0x42083800) → Alloc(addr=0x42087000) → Alloc(addr=0x4208a800) small_fast_free_lists[idx=34, size=0x7000, count=3] → Alloc(addr=0x42047000) → Alloc(addr=0x4204e000) → Alloc(addr=0x42055000) ────────────────────────────────────────────────────────────── Thread 1: active slabs for CoreAlloc(addr=0x42000000) ← LocalAlloc(addr=0x40a42070) ────────────────────────────────────────────────────────────── SlabMetadataCache[idx=34, alloc_size=0x7000, unused=0, length=0]: | → SlabMetadata(addr=0x42000d80, needed=3, sleeping=False, large=False) (count=1) → Alloc(addr=0x42040000) ───────────────────────────────────────────────────────────────── Thread 1: laden for CoreAlloc(addr=0x42000000) ← LocalAlloc(addr=0x40a42070) ───────────────────────────────────────────────────────────────── SlabMetadata(addr=0x42000e80, needed=1, sleeping=True, large=False) (count=0) SlabMetadata(addr=0x42000e00, needed=32, sleeping=True, large=False) (count=0) SlabMetadata(addr=0x42000d00, needed=25, sleeping=True, large=False) (count=0) SlabMetadata(addr=0x42000c80, needed=1, sleeping=True, large=False) (count=0) SlabMetadata(addr=0x42000c00, needed=18, sleeping=True, large=False) (count=0) SlabMetadata(addr=0x42000b80, needed=32, sleeping=True, large=False) (count=0) SlabMetadata(addr=0x42000b00, needed=1, sleeping=True, large=False) (count=0) ───────────────────────────────────────────────────── Thread 1: remote deallocation queue for RemoteAllocator(addr=0x42000800) ← CoreAlloc(addr=0x42000000) ───────────────────────────────────────────────────── [+] Remote deallocation list is empty  snheap chunk \u0026lt;address\u0026gt;: lists details about the Alloc and its slab. If the metaentry has the REMOTE_BACKEND_MARKER bit asserted, that is, the chunk is owned by the backend (not Alloc-bounded), then indicate it as a Chunk. Because backend chunks' metaentry are parsed differently depending on the specific Range, we can make a best guess of the owning Range. In the case that CHERI revocation is enabled, also print whether it is quarantined and its revocation bit value.  gef\u0026gt; snheap chunk 0x42024060 Alloc(addr=0x42024060) Object in quarantine: False Start of object: 0x42024060 Object size: 0x60 Offset into object: 0x0 Metaentry @ 0x50210120 Associated slab details: SlabMetadata(addr=0x42000e00, needed=32, sleeping=True, large=False) (count=0) [+] Free list is empty mrs wrapper CheriBSD 23.11 malloc is shipped with the mrs wrapper around memory allocation APIs: mrs_malloc, mrs_free\u0026hellip;, to enforce heap temporal safety. This plugin provides commands to query for mrs-specific information:\n mrs info: display general information about the mrs quarantine, global state and the revocation bitmap.  gef\u0026gt; mrs info ──────────────────────────────────────────────────────────────────────────────────────────────────── Thread 1 ──────────────────────────────────────────────────────────────────────────────────────────────────── Allocated size: 0x8a90 Max allocated size: 0x8a90 Quarantine size: 0x5100 Quarantine max size: 0x5100 Entire revocation map capability: 0x0000fc0000000000 [rw,0xfc0000000000-0xfe0000000000,len=0x20000000000]  mrs chunk \u0026lt;address\u0026gt;: query whether this chunk is owned by the allocator or quarantined. Also show shadow bitmap offset and value. The information we can query is limited because the capability load generation counter registers are not available to gdb in ring 3, so we can\u0026rsquo;t inspect the kernel internal state of caprevoke unless debugging the kernel or using qemu.  gef\u0026gt; mrs chunk 0x40c40000 Chunk(addr=0x40c40000) In quarantine: Yes Revocation bit address: 0xfc0000818800 Revocation bit set (first word): Yes  mrs quarantine: print the quarantined chunks (and their shadow bit values of the allocation first word).  gef\u0026gt; mrs quarantine application_quarantine[size=0x5100, count=4] → Chunk(addr=0x40c20700, size=0x700) → Chunk(addr=0x40c36000, size=0x400) → Chunk(addr=0x40c40000, size=0x4000) → Chunk(addr=0x40c44000, size=0x600) ", 
            "url": "\/labs\/gef-cheri\/"
        },
        
        
        
        "\/labs\/": {
            
            "title": "Labs",
            "tags": [],
            "content": "", 
            "url": "\/labs\/"
        },
        
        
        
        "\/tags\/": {
            
            "title": "Tags",
            "tags": [],
            "content": "", 
            "url": "\/tags\/"
        },
        
        
        
        "\/tags\/tooling\/": {
            
            "title": "tooling",
            "tags": [],
            "content": "", 
            "url": "\/tags\/tooling\/"
        },
        
        
        
        "\/": {
            
            "title": "Whoami",
            "tags": [],
            "content": "RoundofThree (Zhuo Ying Jiang Li)  Email: zyj20 [at] cl.cam.ac.uk Github resume.pdf (outdated)  Introduction Who am I?\n A security and systems enthusiast A salty tuna ACG lover Currently a PhD student at the University of Cambridge. My research areas span computer security and systems.  Skills  Vulnerability research and exploitation OS internals (CheriBSD, FreeBSD, Linux)  Projects  Security implications of CHERI on LLVM compiler optimizations and verification  Extend Alive2 to reason about CHERI semantics   GEF with Morello support GEF extra plugins  jemalloc heap analysis snmalloc heap analysis   Ghidra with Morello support Unicorn with Morello support  ", 
            "url": "\/"
        },
        
        
        
        "\/research\/snmalloc\/": {
            
            "title": "Exploring snmalloc internals",
            "tags": ["malloc",],
            "content": "Note: this is an updating post.\nIntroduction snmalloc is a memory allocator by Microsoft Research that uses a \u0026ldquo;message passing\u0026rdquo; scheme. You can find its source code here. It is designed to be performant in highly parallel workloads where memory allocated in one thread is typically deallocated in another thread. A nice catch is that snmalloc is highly customizable, and more to my interest, security mitigations can be customized. It also provides abstraction layers for different architectures (AAL) and platforms (PAL).\nsnmalloc is available as an allocator in CheriBSD, which is a fork of FreeBSD with CHERI support. Currently, jemalloc is still the default allocator in FreeBSD but we can build with LIBC_MALLOC=snmalloc make options to try snmalloc out.\nThis article documents my exploration of the internals of snmalloc as available in its official repository.\nCustom configuration The default behaviour of the snmalloc global memory allocator is defined by the StandardConfig class in backend/globalconfig.h but it can be customized by defining the SNMALLOC_PROVIDE_OWN_CONFIG macro and exporting a customized allocator type as snmalloc::Alloc, as in:\n// backend/globalconfig.h  /** * Create allocator type for this configuration. */ using Alloc = snmalloc::LocalAllocator\u0026lt;snmalloc::StandardConfig\u0026gt;; The configuration class defines the implementation of Pagemap and Authmap (only relevant to architectures that support strict provenance, for now just CHERI), the Backend and the LocalState.\nWe will assume the given standard config in this article.\nFrontend vs backend There are many mentions of the concepts of frontend and backend in the (quite detailed) documentation in snmalloc\u0026rsquo;s repository. In simple terms, a frontend allocator gets raw memory chunks from the backend and assigns allocator-specific metadata to them, like sizeclass and slab information. The backend interfaces with the kernel with memory-mapping operations and keeps track of memory chunks to service the frontend.\nThe backend behavior is defined by the Backend class in the configuration. The frontend behavior is defined by the Alloc class exported by the configuration, which is a LocalAllocator. Each local allocator has an associated core allocator core_alloc of type CoreAlloc and a cache local_cache to hold freelists of small sizeclass.\n// mem/localalloc.h  template\u0026lt;SNMALLOC_CONCEPT(IsConfig) Config_\u0026gt; class LocalAllocator { // [...]  private: // [...]  // Free list per small size class. These are used for  // allocation on the fast path. This part of the code is inspired by  // mimalloc.  // Also contains remote deallocation cache.  LocalCache local_cache{\u0026amp;Config::unused_remote}; // Underlying allocator for most non-fast path operations.  CoreAlloc* core_alloc{nullptr}; // [...]  } The exposed API to the user, eg. malloc, free, realloc, are wrappers around the ThreadAlloc class, which is in turn a wrapper around Alloc (see global/global.h and override/override.h). ThreadAlloc is used to hold a thread-local allocator of configurable type Alloc and the thread-local state. This is similar to tcaches in Linux glibc malloc: each thread has their own local allocator object with their own small_fast_free_lists in local_cache. While local allocators are associated with one core allocator, a core allocator can be associated with many local allocators of different threads, in which case they are sharing the same global state (similar to how different threads share the same fastbins, small_bins, unsorted_bins and large_bins in glibc).\nApart from ThreadAlloc, ScopedAllocator is another wrapper around Alloc that doesn\u0026rsquo;t depend on thread-local storage, so it can be used as a bootstrapping (slow) allocator, as in test/func/thread_alloc_external/thread_alloc_external.cc.\nFrontend allocation Because the snmalloc code base uses CapPtr\u0026lt;T, B\u0026gt; heavily to annotate pointers to heap memory with bounding, we will use the same terminology to refer to the memory allocations. That is, a certain allocation can be a:\n Alloc: this is owned by the frontend Chunk: the backend manages chunk allocations and returns them to the frontend. This is what I called raw chunks. The frontend assigns metadata to chunks and converts them into allocs Arena: only owned by the backend, which are then further refined into chunks  // ds_core/ptrwrap.h  namespace bounds { /** * Internal access to an entire Arena. These exist only in the backend. */ using Arena = bound\u0026lt; dimension::Spatial::Arena, dimension::AddressSpaceControl::Full, dimension::Wildness::Tame\u0026gt;; /** * Internal access to a Chunk of memory. These flow across the boundary * between back- and front-ends, for example. */ using Chunk = bound\u0026lt; dimension::Spatial::Chunk, dimension::AddressSpaceControl::Full, dimension::Wildness::Tame\u0026gt;; /** * User access to an entire Chunk. Used as an ephemeral state when * returning a large allocation. See capptr_chunk_is_alloc. */ using ChunkUser = Chunk::with_address_space_control\u0026lt;dimension::AddressSpaceControl::User\u0026gt;; /** * Internal access to just one allocation (usually, within a slab). */ using AllocFull = Chunk::with_spatial\u0026lt;dimension::Spatial::Alloc\u0026gt;; /** * User access to just one allocation (usually, within a slab). */ using Alloc = AllocFull::with_address_space_control\u0026lt; dimension::AddressSpaceControl::User\u0026gt;; /** * A wild (i.e., putative) CBAllocExport pointer handed back by the * client. See capptr_from_client() and capptr_domesticate(). */ using AllocWild = Alloc::with_wildness\u0026lt;dimension::Wildness::Wild\u0026gt;; } // namespace bounds I know it gets confusing, especially if you have familiarity with other memory allocators that use the same name to refer to different concepts. Shikata ga nai, hopefully I\u0026rsquo;m clear enough and not confused myself.\nAllocators As I said, a malloc request first reaches the local allocator. The local allocator is said to handle fast path allocations as it contains freelists in its local cache that store previously freed allocs (think glibc tcache). If there are no allocs of the requested size, it delegates the allocation request to its core allocator and that is slow path allocation. Some key functions exposed by LocalAllocator are:\n alloc  alloc_not_small: for large requests, request a chunk from the backend and insert into the core allocator laden list of inactive slabs. small_alloc: for requests that are representable in a small sizeclass, first check the local cache. If there are no entries in the small fast freelist, invoke the core allocator small_alloc.   dealloc: deallocation is delegated to the core allocator  A core allocator is a stateful allocator containing slabs alloc_classes and other fields like laden, entropy, backend_state, attached_cache. Slabs are fixed-size allocations that can be split into smaller. It interfaces with the backend to expose a higher level message object allocation API:\n small_alloc: check for a slab of the requested small sizeclass and get an alloc from the slab. Then update the slab state to inactive if it becomes inactive (moved to laden). BackendSlabMetadata::alloc_free_list fills the small fast freelists in the local cache with the remaining elements in the slab.  small_alloc_slow: if there is no slab that serves allocs of the requested size, request the backend for a chunk to construct a new slab.   dealloc_local_object  dealloc_local_object_fast: the alloc is inserted into its associated slab freelist. dealloc_local_object_slow: the slow path is triggered when the associated slab has needed_ == 0. If the case of:  A large alloc: the chunk is returned to the backend (dealloc_chunk) A sleeping slab: the addition of a freed object to its freelist hits the threshold to wake up the slab to start servicing requests An unused slab after this deallocation: this can trigger the deallocation of unused slabs of the same sizeclass if there are enough of them      /** * The number of deallocation required until we hit a slow path. This * counts down in two different ways that are handled the same on the * fast path. The first is * - deallocations until the slab has sufficient entries to be considered * useful to allocate from. This could be as low as 1, or when we have * a requirement for entropy then it could be much higher. * - deallocations until the slab is completely unused. This is needed * to be detected, so that the statistics can be kept up to date, and * potentially return memory to the a global pool of slabs/chunks. */ uint16_t needed_ = 0; This is a snipped of the core allocator fields.\n// class CoreAllocator // ...  /** * Define local names for specialised versions of various types that are * specialised for the back-end that we are using. * @{ */ using BackendSlabMetadata = typename Config::Backend::SlabMetadata; using PagemapEntry = typename Config::PagemapEntry; /// }@  /** * Per size class list of active slabs for this allocator. */ struct SlabMetadataCache { SeqSet\u0026lt;BackendSlabMetadata\u0026gt; available{}; uint16_t unused = 0; uint16_t length = 0; } alloc_classes[NUM_SMALL_SIZECLASSES]{}; /** * The set of all slabs and large allocations * from this allocator that are full or almost full. */ SeqSet\u0026lt;BackendSlabMetadata\u0026gt; laden{}; // ... As snmalloc is a message passing allocator, you can expect other message queue operations: init_message_queue, handle_message_queue, post, flush\u0026hellip; that I won\u0026rsquo;t dig in just now.\nAdditionally, each local allocator is associated with a remote allocator. It represents a message queue of freed objects so that an object can be allocated by one allocator and deallocated by a different allocator in a message passing fashion.\n// mem/remoteallocator.h  struct alignas(REMOTE_MIN_ALIGN) RemoteAllocator { /** * Global key for all remote lists. * * Note that we use a single key for all remote free lists and queues. * This is so that we do not have to recode next pointers when sending * segments, and look up specific keys based on destination. This is * potentially more performant, but could make it easier to guess the key. */ inline static FreeListKey key_global{0xdeadbeef, 0xbeefdead, 0xdeadbeef}; using alloc_id_t = address_t; // Store the message queue on a separate cacheline. It is mutable data that  // is read by other threads.  alignas(CACHELINE_SIZE) freelist::AtomicQueuePtr back{nullptr}; // Store the two ends on different cache lines as access by different  // threads.  alignas(CACHELINE_SIZE) freelist::AtomicQueuePtr front{nullptr}; // Fake first entry  freelist::Object::T\u0026lt;capptr::bounds::AllocWild\u0026gt; stub{}; // [...]  } MetaEntry vs SlabMetadata Every allocation, be it owned by frontend or backend, has a metaentry stored in the pagemap. However, allocs in the frontend have additional metadata related to the slab and they are stored in SlabMetadata objects. Backend chunks don\u0026rsquo;t have this same slab metadata because this slab representation is specific to the frontend.\nMetaentries are stored in a pagemap. The default FlatPagemap implementation has an array of pointers to metaentries (body).\n// ds/pagemap.h  /** * Simple pagemap that for each GRANULARITY_BITS of the address range * stores a T. */ template\u0026lt;size_t GRANULARITY_BITS, typename T, typename PAL, bool has_bounds\u0026gt; class FlatPagemap { public: static constexpr size_t SHIFT = GRANULARITY_BITS; static constexpr size_t GRANULARITY = bits::one_at_bit(GRANULARITY_BITS); private: /** * Before init is called will contain a single entry * that is the default value. This is needed so that * various calls do not have to check for nullptr. * free(nullptr) * and * malloc_usable_size(nullptr) * do not require an allocation to have ocurred before * they are called. */ inline static const T default_value{}; /** * The representation of the page map. * * Initially a single element to ensure nullptr operations * work. */ T* body{const_cast\u0026lt;T*\u0026gt;(\u0026amp;default_value)}; /** * The representation of the pagemap, but nullptr if it has not been * initialised. Used to combine init checking and lookup. */ T* body_opt{nullptr}; /** * If `has_bounds` is set, then these should contain the * bounds of the heap that is being managed by this pagemap. */ address_t base{0}; size_t size{0}; // [...]  } A metaentry consists of two fields of the size of a pointer of the target architecture: meta and remote_and_sizeclass (abbreviated ras). For an alloc, meta points to its SlabMetadata object, but for chunks in the backend, it can have other meanings. For an alloc, ras encodes a reference to the owning frontend allocator\u0026rsquo;s message queue RemoteAllocator and the sizeclass. Again, this field has a different meaning for backend owned chunks.\nclass MetaEntryBase { protected: // [...]  /** * In common cases, the pointer to the slab metadata. See * docs/AddressSpace.md for additional details. * * The bottom bit is used to indicate if this is the first chunk in a PAL * allocation, that cannot be combined with the preceeding chunk. */ uintptr_t meta{0}; /** * In common cases, a bit-packed pointer to the owning allocator (if any), * and the sizeclass of this chunk. See `encode` for * details of this case and docs/AddressSpace.md for further details. */ uintptr_t remote_and_sizeclass{0}; // [...]  }; Every alloc has a slabmetadata object which links it to a slab. A slab consists of a freelist (or two in case the random_preserve mitigation is enabled) of freed allocs.\n// [...]  /** * Data-structure for building the free list for this slab. */ SNMALLOC_NO_UNIQUE_ADDRESS freelist::Builder\u0026lt;mitigations(random_preserve)\u0026gt; free_queue; /** * The number of deallocation required until we hit a slow path. This * counts down in two different ways that are handled the same on the * fast path. The first is * - deallocations until the slab has sufficient entries to be considered * useful to allocate from. This could be as low as 1, or when we have * a requirement for entropy then it could be much higher. * - deallocations until the slab is completely unused. This is needed * to be detected, so that the statistics can be kept up to date, and * potentially return memory to the a global pool of slabs/chunks. */ uint16_t needed_ = 0; /** * Flag that is used to indicate that the slab is currently not active. * I.e. it is not in a CoreAllocator cache for the appropriate sizeclass. */ bool sleeping_ = false; /** * Flag to indicate this is actually a large allocation rather than a slab * of small allocations. */ bool large_ = false; // [...] Backend allocation The backend manages memory allocations from the platform. This layer deals with Chunk-bounded pointers capptr::Chunk\u0026lt;void\u0026gt;. For every object allocation request, the frontend actually asks for two chunks, one for the slab metadata and one for the user data chunk. Some key functions are:\n alloc_meta_data dealloc_meta_data alloc_chunk dealloc_chunk  /** * This class implements the standard backend for handling allocations. * It is parameterised by its Pagemap management and * address space management (LocalState). */ template\u0026lt; SNMALLOC_CONCEPT(IsPAL) PAL, typename PagemapEntry, typename Pagemap, typename Authmap, typename LocalState\u0026gt; class BackendAllocator The metadata and the user data chunks are allocated differently. Metadata chunks are serviced by local_state-\u0026gt;get_meta_range() while user data chunks are serviced by local_state-\u0026gt;get_object_range(). What are these ranges? Ranges are similar to passes in compilers: it takes a pointer to a chunk and applies a set of operations to it before returning it. A range exposes the APIs alloc_range and dealloc_range.\nLocalState contains information about the ranges used by a backend allocator. For example, the default LocalState is defined as:\nusing LocalState = std::conditional_t\u0026lt; mitigations(metadata_protection), MetaProtectedRangeLocalState\u0026lt;Pal, Pagemap, Base\u0026gt;, StandardLocalState\u0026lt;Pal, Pagemap, Base\u0026gt;\u0026gt;; And StandardLocalState exposes get_object_range and get_meta_range, so that user chunks are allocated through LargeObjectRange while metadata requests are allocated through ObjectRange (SmallBuddyRange then LargeObjectRange).\n// ...  private: using ObjectRange = Pipe\u0026lt;LargeObjectRange, SmallBuddyRange\u0026gt;; ObjectRange object_range; public: // Expose a global range for the initial allocation of meta-data.  using GlobalMetaRange = Pipe\u0026lt;ObjectRange, GlobalRange\u0026gt;; /** * Where we turn for allocations of user chunks. * * Reach over the SmallBuddyRange that\u0026#39;s at the near end of the ObjectRange * pipe, rather than having that range adapter dynamically branch to its * parent. */ LargeObjectRange* get_object_range() { return object_range.template ancestor\u0026lt;LargeObjectRange\u0026gt;(); } /** * The backend has its own need for small objects without using the * frontend allocators; this range manages those. */ ObjectRange\u0026amp; get_meta_range() { // Use the object range to service meta-data requests.  return object_range; } // ... Other concepts Other concepts (I will dig into in the future):\n Domestication: see capptr_domesticate. This concerns inter-thread memory allocation requests. Authmap: this is used to enforce strict provenance in AALs that support strict provenance (currently only AALs with AAL_CHERI mixin).  Freelist and SeqSet structs Snmalloc uses four iterable structures to link objects.\nFreelist Builder A builder can contain two freelist queues depending on whether random_preserve mitigation is enabled. As per the comments, it is used to build a freelist in object space.\n  What is the structure of a freelist builder?\n head: pointer to the first element, aka FreelistPtr end: pointer to pointer to the last element, aka pointer to FreelistPtr    What uses a freelist builder?\n FrontendSlabMetadata-\u0026gt;free_queue keeps track of active Allocs under the slab RemoteDeallocCache-\u0026gt;lists is an array of Builder    How is it iterated?\n Empty case: end == \u0026amp;head First element: curr = head Next element: curr.next_object Last element: curr == end (curr included)    Freelist Iter Used to iterate a freelist in object space.\n  What is the structure of a freelist iter?\n curr: freelist pointer, aka pointer to the first element    What uses a freelist iter?\n LocalCache-\u0026gt;small_fast_free_lists is a primitive array of freelist iters    How is it iterated?\n Empty case: curr == nullptr First element: curr Next element: curr.next_object Last element: right before the empty case (null terminator)    SeqSet (sequential set) Doubly-linked cyclic list linked using T::node field. Used to group slab metadata in metadata space (not used in object space).\n  What is the structure of a seqset?\n head: a SeqSetNode (containing pointers next and prev)    What uses a seqset?\n Core allocator laden groups FrontendSlabMetadata Core allocator alloc_classes contained available which groups FrontendSlabMetadata    How is it iterated?\n Empty case: head.next == \u0026amp;head First case: curr = head.next Next case: curr.next (or curr.prev) Last case: right before the empty case    Remote deallocation queue The remote deallocation queue uses a different iterable struct.\n  What is the structure of a remote deallocation queue?\n back: freelist pointer front: freelist pointer stub: fake first entry    How is it iterated?\n Empty case: front == \u0026amp;stub or back == nullptr First element: curr = front Next element: curr.next_object Last element: right before curr == nullptr or curr == back (curr not included)    Security checks The security checks are nicely grouped in the ds_core/mitigations.h header file. One future research question would be, how do these security mitigations affect the exploitability of memory safety bugs? One could test the added exploit constraints under different combinations of enabled security mitigations.\n/** * Randomize the location of the pagemap within a larger address space * allocation. The other pages in that allocation may fault if accessed, on * platforms that can efficiently express such configurations. * * This guards against adversarial attempts to access the pagemap. * * This is unnecessary on StrictProvenance architectures. */ constexpr mitigation::type random_pagemap{1 \u0026lt;\u0026lt; 0}; /** * Ensure that every slab (especially slabs used for larger \u0026#34;small\u0026#34; size * classes) has a larger minimum number of objects and that a larger * percentage of objects in a slab must be free to awaken the slab. * * This should frustrate use-after-reallocation attacks by delaying reuse. * When combined with random_preserve, below, it additionally ensures that at * least some shuffling of free objects is possible, and, hence, that there * is at least some unpredictability of reuse. * * TODO: should this be split? mjp: Would require changing some thresholds. * The min waking count needs to be ensure we have enough objects on a slab, * hence is related to the min count on a slab. Currently we without this, we * have min count of slab of 16, and a min waking count with this enabled * of 32. So we would leak memory. */ constexpr mitigation::type random_larger_thresholds{1 \u0026lt;\u0026lt; 1}; /** * * Obfuscate forward-edge pointers in intra-slab free lists. * * This helps prevent a UAF write from re-pointing the free list arbitrarily, * as the de-obfuscation of a corrupted pointer will generate a wild address. * * This is not available on StrictProvenance architectures. */ constexpr mitigation::type freelist_forward_edge{1 \u0026lt;\u0026lt; 2}; /** * Store obfuscated backward-edge addresses in intra-slab free lists. * * Ordinarily, these lists are singly-linked. Storing backward-edges allows * the allocator to verify the well-formedness of the links and, importantly, * the acyclicity of the list itself. These backward-edges are also * obfuscated in an attempt to frustrate an attacker armed with UAF * attempting to construct a new well-formed list. * * Because the backward-edges are not traversed, this is available on * StrictProvenance architectures, unlike freelist_forward_edge. * * This is required to detect double frees as it will break the doubly linked * nature of the free list. */ constexpr mitigation::type freelist_backward_edge{1 \u0026lt;\u0026lt; 3}; /** * When de-purposing a slab (releasing its address space for reuse at a * different size class or allocation), walk the free list and validate the * domestication of all nodes along it. * * If freelist_forward_edge is also enabled, this will probe the * domestication status of the de-obfuscated pointers before traversal. * Each of domestication and traversal may probabilistically catch UAF * corruption of the free list. * * If freelist_backward_edge is also enabled, this will verify the integrity * of the free list links. * * This gives the allocator \u0026#34;one last chance\u0026#34; to catch UAF corruption of a * slab\u0026#39;s free list before the slab is de-purposed. * * This is required to comprehensively detect double free. */ constexpr mitigation::type freelist_teardown_validate{1 \u0026lt;\u0026lt; 4}; /** * When initializing a slab, shuffle its free list. * * This guards against attackers relying on object-adjacency or address-reuse * properties of the allocation stream. */ constexpr mitigation::type random_initial{1 \u0026lt;\u0026lt; 5}; /** * When a slab is operating, randomly assign freed objects to one of two * intra-slab free lists. When selecting a slab\u0026#39;s free list for allocations, * select the longer of the two. * * This guards against attackers relying on object-adjacency or address-reuse * properties of the allocation stream. */ constexpr mitigation::type random_preserve{1 \u0026lt;\u0026lt; 6}; /** * Randomly introduce another slab for a given size-class, rather than use * the last available to an allocator. * * This guards against attackers relying on address-reuse, especially in the * pathological case of a size-class having only one slab with free entries. */ constexpr mitigation::type random_extra_slab{1 \u0026lt;\u0026lt; 7}; /** * Use a LIFO queue, rather than a stack, of slabs with free elements. * * This generally increases the time between address reuse. */ constexpr mitigation::type reuse_LIFO{1 \u0026lt;\u0026lt; 8}; /** * This performs a variety of inexpensive \u0026#34;sanity\u0026#34; tests throughout the * allocator: * * - Requests to free objects must * - not be interior pointers * - be of allocated address space * - Requests to free objects which also specify the size must specify a size * that agrees with the current allocation. * * This guards gainst various forms of client misbehavior. * * TODO: Should this be split? mjp: It could, but let\u0026#39;s not do this until * we have performance numbers to see what this costs. */ constexpr mitigation::type sanity_checks{1 \u0026lt;\u0026lt; 9}; /** * On CHERI, perform a series of well-formedness tests on capabilities given * when requesting to free an object. */ constexpr mitigation::type cheri_checks{1 \u0026lt;\u0026lt; 10}; /** * Erase intra-slab free list metadata before completing an allocation. * * This mitigates information disclosure. */ constexpr mitigation::type clear_meta{1 \u0026lt;\u0026lt; 11}; /** * Protect meta data blocks by allocating separate from chunks for * user allocations. This involves leaving gaps in address space. * This is less efficient, so should only be applied for the checked * build. */ constexpr mitigation::type metadata_protection{1 \u0026lt;\u0026lt; 12}; /** * If this mitigation is enabled, then Pal implementations should provide * exceptions/segfaults if accesses do not obey the * - using * - using_readonly * - not_using * model. */ static constexpr mitigation::type pal_enforce_access{1 \u0026lt;\u0026lt; 13}; GEF plugin for snmalloc I have developed a GEF plugin that adds a heap manager that understands snmalloc heap. It is limited to the default configuration and the default set of security mitigations. You can read more in this other blog post or check it out.\nNote: the plugin requires GEF with CHERI support because it assumes a separation of address space size (adrsize) and pointer size (ptrsize). The original GEF assumes adrsize == ptrsize but this is not true for CHERI-enabled architectures.\n", 
            "url": "\/research\/snmalloc\/"
        },
        
        
        
        "\/tags\/malloc\/": {
            
            "title": "malloc",
            "tags": [],
            "content": "", 
            "url": "\/tags\/malloc\/"
        },
        
        
        
        "\/research\/": {
            
            "title": "Researches",
            "tags": [],
            "content": "", 
            "url": "\/research\/"
        },
        
        
        
        "\/categories\/": {
            
            "title": "Categories",
            "tags": [],
            "content": "", 
            "url": "\/categories\/"
        },
        
        
        
        "\/search\/": {
            
            "title": "Search",
            "tags": [],
            "content": "", 
            "url": "\/search\/"
        },
        
    }
</script>
<script src="/assets/lunr.min.js"></script>
<script src="/assets/search.js"></script>
</div>
<footer class="footer">
<div class="footer__inner">
<div class="copyright copyright--user">
<span>© 2024 RoundofThree</span>
</div>
</div>
</footer>
<script src="/assets/main.js"></script>
<script src="/assets/prism.js"></script>
</div>
</body>
</html>
